\chapter{Discussion}
Although no large feed, we showed that our agents can comfortably beat the Random agent.
The Greedy and Heuristic agent beat them convincingly and there is little to analayse there.\\
The approach of seperating the training for each contract was also not successful, but this should probably be
attributed to the difference in batch size.
Although there is a clear relationship between batch size and performance, experiments showed that this relationship
is not linear.
\newline
The results are somewhat disappointing and there are a number of reasons for that.\\
The biggest flaw in our implementation was the use of the trick history as we only passed the played cards a player
previously played, but any good player will tell you that that is only somewhat useful.
The order of cards matters, since it reveals useful information about the game, as shown by the Heuristic agent which
makes use of the played cards to at least count trump and identify his partner.\\
Whether an opponent does not hold trump anymore, can be only inferred from the trick history.
However, to fully represent the trick history we would need to increase the input vector by a factor of almost
10.
Each card is represented using a 32bit vector, plus a 4 bit position vector, indicating who that card belongs to, and
another 4 bit vector to represent the lead, multiplied by 8 Tricks).
For the scope of this project and the resources available this seemed too much.
\newline
\newline
Another limitation might also be the network size itself.
In previous experiments we originally started with an extra hidden layer and 64 neuron for each hidden layer, but
convergence was only possible with 150000 hands per episode, which would require an overhaul of our trainings
pipeline, to include parallel data generation.
\newline
Lastly we became critical about the idea of static bidding, which is on the one hand really useful for the purpose of
this work, as it allows clear comparison between agents, but might hinder training progress.
The static bidding only returned Solo hands, that were not just playable but even easily winning.
On the one hand, this allows our network to generalize a winning strategy with ease, but the bar might also be not very
high.
\newline
Implementing the bidding into a network would certainly be interesting, but the results would be harder to interpret
since the baseline agents would use a different hand selection completely.\\
Also it is unclear how the bidding would interact with the actual playing, because the fastest was to lose a lot of
points in Schafkopf is losing Solo contracts.
During exploration the network would inevitably choose losing contracts, which in turn might nullify any learning
effect since every action is losing regardless of play.\\
A solution to this might be to use a separate network for the bidding, that is either trained from real games using
imitation learning.
Another way might be to train the bidding and playing network alternating turns, allowing one network to catch up
with the other one, without polluting each other's training data with noise.
\newline
Lastly it has to be sad that \textbf{PPO} proved to very robust and there any change in parameters was not
noticeable during training.
The biggest factor was batch size, which when increased would lead almost guarantee convergence in our case.\\
There was however a limit to that, where once a models converged, they did not show any improvement in playing strength.


\section{Future Work}
In the future we would like to revisit larger networks that make full use of the trick history, as we see this as one
of the main reason for our relatively poor results.
Additionally, we would like to explore implementing a bidding network, which is trained on human play data.
