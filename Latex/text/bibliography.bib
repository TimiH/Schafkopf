% Encoding: UTF-8

@Article{Niklaus2019,
  author        = {Joel Niklaus and Michele Alberti and Vinaychandran Pondenkandath and Rolf Ingold and Marcus Liwicki},
  journal       = {6th Swiss Conference on Data Science (SDS), Bern, Switzerland, 2019},
  title         = {Survey of Artificial Intelligence for Card Games and Its Application to the Swiss Game Jass},
  year          = {2019},
  month         = jun,
  abstract      = {In the last decades we have witnessed the success of applications of Artificial Intelligence to playing games. In this work we address the challenging field of games with hidden information and card games in particular. Jass is a very popular card game in Switzerland and is closely connected with Swiss culture. To the best of our knowledge, performances of Artificial Intelligence agents in the game of Jass do not outperform top players yet. Our contribution to the community is two-fold. First, we provide an overview of the current state-of-the-art of Artificial Intelligence methods for card games in general. Second, we discuss their application to the use-case of the Swiss card game Jass. This paper aims to be an entry point for both seasoned researchers and new practitioners who want to join in the Jass challenge.},
  archiveprefix = {arXiv},
  comment       = {Talks about the different strategies that exist for imperfect information games.},
  eprint        = {1906.04439},
  file          = {:http\://arxiv.org/pdf/1906.04439v1:PDF},
  keywords      = {cs.AI},
  primaryclass  = {cs.AI},
}

@Article{Schulman2017,
  author        = {John Schulman and Filip Wolski and Prafulla Dhariwal and Alec Radford and Oleg Klimov},
  title         = {Proximal Policy Optimization Algorithms},
  year          = {2017},
  month         = jul,
  abstract      = {We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a "surrogate" objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.},
  archiveprefix = {arXiv},
  eprint        = {1707.06347},
  file          = {:http\://arxiv.org/pdf/1707.06347v2:PDF},
  keywords      = {cs.LG},
  primaryclass  = {cs.LG},
}

@Article{Charlesworth2018,
  author        = {Henry Charlesworth},
  title         = {Application of Self-Play Reinforcement Learning to a Four-Player Game of Imperfect Information},
  year = {2018},
  month = aug,
  abstract = {We introduce a new virtual environment for simulating a card game known as "Big 2". This is a four-player game of imperfect information with a relatively complicated action space (being allowed to play 1,2,3,4 or 5 card combinations from an initial starting hand of 13 cards). As such it poses a challenge for many current reinforcement learning methods. We then use the recently proposed "Proximal Policy Optimization" algorithm to train a deep neural network to play the game, purely learning via self-play, and find that it is able to reach a level which outperforms amateur human players after only a relatively short amount of training time and without needing to search a tree of future game states.},
  archiveprefix = {arXiv},
  eprint = {1808.10442},
  file = {:http\://arxiv.org/pdf/1808.10442v1:PDF},
  keywords = {cs.LG, cs.AI, stat.ML},
  primaryclass = {cs.LG},
}

@Article{,
title = {https://medium.com/aureliantactics/ppo-hyperparameters-and-ranges-6fc2d29bccbe},
}

@Article{Bansal2017,
author = {Trapit Bansal and Jakub Pachocki and Szymon Sidor and Ilya Sutskever and Igor Mordatch},
title = {Emergent Complexity via Multi-Agent Competition},
year = {2017},
month = oct,
abstract = {Reinforcement learning algorithms can train agents that solve problems in complex, interesting environments. Normally, the complexity of the trained agent is closely related to the complexity of the environment. This suggests that a highly capable agent requires a complex environment for training. In this paper, we point out that a competitive multi-agent environment trained with self-play can produce behaviors that are far more complex than the environment itself. We also point out that such environments come with a natural curriculum, because for any skill level, an environment full of agents of this level will have the right level of difficulty. This work introduces several competitive multi-agent environments where agents compete in a 3D world with simulated physics. The trained agents learn a wide variety of complex and interesting skills, even though the environment themselves are relatively simple. The skills include behaviors such as running, blocking, ducking, tackling, fooling opponents, kicking, and defending using both arms and legs. A highlight of the learned behaviors can be found here: https://goo.gl/eR7fbX},
archiveprefix = {arXiv},
eprint = {1710.03748},
file = {:http\://arxiv.org/pdf/1710.03748v3:PDF},
keywords = {cs.AI},
primaryclass = {cs.AI},
}

@Comment{jabref-meta: databaseType:bibtex;}
