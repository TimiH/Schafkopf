% Encoding: UTF-8

@Article{Niklaus2019,
  author        = {Joel Niklaus and Michele Alberti and Vinaychandran Pondenkandath and Rolf Ingold and Marcus Liwicki},
  journal       = {6th Swiss Conference on Data Science (SDS), Bern, Switzerland, 2019},
  title         = {Survey of Artificial Intelligence for Card Games and Its Application to the Swiss Game Jass},
  year          = {2019},
  month         = jun,
  abstract      = {In the last decades we have witnessed the success of applications of Artificial Intelligence to playing games. In this work we address the challenging field of games with hidden information and card games in particular. Jass is a very popular card game in Switzerland and is closely connected with Swiss culture. To the best of our knowledge, performances of Artificial Intelligence agents in the game of Jass do not outperform top players yet. Our contribution to the community is two-fold. First, we provide an overview of the current state-of-the-art of Artificial Intelligence methods for card games in general. Second, we discuss their application to the use-case of the Swiss card game Jass. This paper aims to be an entry point for both seasoned researchers and new practitioners who want to join in the Jass challenge.},
  archiveprefix = {arXiv},
  comment       = {Talks about the different strategies that exist for imperfect information games.},
  eprint        = {1906.04439},
  file          = {:http\://arxiv.org/pdf/1906.04439v1:PDF},
  keywords      = {cs.AI},
  primaryclass  = {cs.AI},
}

@Article{Schulman2017,
  author        = {John Schulman and Filip Wolski and Prafulla Dhariwal and Alec Radford and Oleg Klimov},
  title         = {Proximal Policy Optimization Algorithms},
  year          = {2017},
  month         = jul,
  abstract      = {We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a "surrogate" objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.},
  archiveprefix = {arXiv},
  eprint        = {1707.06347},
  file          = {:http\://arxiv.org/pdf/1707.06347v2:PDF},
  keywords      = {cs.LG},
  primaryclass  = {cs.LG},
}

@Article{Charlesworth2018,
  author        = {Henry Charlesworth},
  title         = {Application of Self-Play Reinforcement Learning to a Four-Player Game of Imperfect Information},
  year = {2018},
  month = aug,
  abstract = {We introduce a new virtual environment for simulating a card game known as "Big 2". This is a four-player game of imperfect information with a relatively complicated action space (being allowed to play 1,2,3,4 or 5 card combinations from an initial starting hand of 13 cards). As such it poses a challenge for many current reinforcement learning methods. We then use the recently proposed "Proximal Policy Optimization" algorithm to train a deep neural network to play the game, purely learning via self-play, and find that it is able to reach a level which outperforms amateur human players after only a relatively short amount of training time and without needing to search a tree of future game states.},
  archiveprefix = {arXiv},
  eprint = {1808.10442},
  file = {:http\://arxiv.org/pdf/1808.10442v1:PDF},
  keywords = {cs.LG, cs.AI, stat.ML},
  primaryclass = {cs.LG},
}

@Article{,
title = {https://medium.com/aureliantactics/ppo-hyperparameters-and-ranges-6fc2d29bccbe},
}

@Article{Bansal2017,
author = {Trapit Bansal and Jakub Pachocki and Szymon Sidor and Ilya Sutskever and Igor Mordatch},
title = {Emergent Complexity via Multi-Agent Competition},
year = {2017},
month = oct,
abstract = {Reinforcement learning algorithms can train agents that solve problems in complex, interesting environments. Normally, the complexity of the trained agent is closely related to the complexity of the environment. This suggests that a highly capable agent requires a complex environment for training. In this paper, we point out that a competitive multi-agent environment trained with self-play can produce behaviors that are far more complex than the environment itself. We also point out that such environments come with a natural curriculum, because for any skill level, an environment full of agents of this level will have the right level of difficulty. This work introduces several competitive multi-agent environments where agents compete in a 3D world with simulated physics. The trained agents learn a wide variety of complex and interesting skills, even though the environment themselves are relatively simple. The skills include behaviors such as running, blocking, ducking, tackling, fooling opponents, kicking, and defending using both arms and legs. A highlight of the learned behaviors can be found here: https://goo.gl/eR7fbX},
archiveprefix = {arXiv},
eprint = {1710.03748},
file = {:http\://arxiv.org/pdf/1710.03748v3:PDF},
keywords = {cs.AI},
primaryclass = {cs.AI},
}

@Article{Smith2017,
author = {Leslie N. Smith and Nicholay Topin},
title = {Super-Convergence: Very Fast Training of Neural Networks Using Large Learning Rates},
year = {2017},
month = aug,
abstract = {In this paper, we describe a phenomenon, which we named "super-convergence", where neural networks can be trained an order of magnitude faster than with standard training methods. The existence of super-convergence is relevant to understanding why deep networks generalize well. One of the key elements of super-convergence is training with one learning rate cycle and a large maximum learning rate. A primary insight that allows super-convergence training is that large learning rates regularize the training, hence requiring a reduction of all other forms of regularization in order to preserve an optimal regularization balance. We also derive a simplification of the Hessian Free optimization method to compute an estimate of the optimal learning rate. Experiments demonstrate super-convergence for Cifar-10/100, MNIST and Imagenet datasets, and resnet, wide-resnet, densenet, and inception architectures. In addition, we show that super-convergence provides a greater boost in performance relative to standard training when the amount of labeled training data is limited. The architectures and code to replicate the figures in this paper are available at github.com/lnsmith54/super-convergence. See http://www.fast.ai/2018/04/30/dawnbench-fastai/ for an application of super-convergence to win the DAWNBench challenge (see https://dawn.cs.stanford.edu/benchmark/).},
archiveprefix = {arXiv},
eprint = {1708.07120},
file = {:http\://arxiv.org/pdf/1708.07120v3:PDF},
keywords = {cs.LG, cs.CV, cs.NE, stat.ML},
primaryclass = {cs.LG},
}

@Article{krizhevsky2012imagenet,
author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
journal = {Advances in neural information processing systems},
title = {Imagenet classification with deep convolutional neural networks},
year = {2012},
pages = {1097--1105},
volume = {25},
}

@Article{Ilyas2018,
author = {Andrew Ilyas and Logan Engstrom and Shibani Santurkar and Dimitris Tsipras and Firdaus Janoos and Larry Rudolph and Aleksander Madry},
title = {A Closer Look at Deep Policy Gradients},
year = {2018},
month = nov,
abstract = {We study how the behavior of deep policy gradient algorithms reflects the conceptual framework motivating their development. To this end, we propose a fine-grained analysis of state-of-the-art methods based on key elements of this framework: gradient estimation, value prediction, and optimization landscapes. Our results show that the behavior of deep policy gradient algorithms often deviates from what their motivating framework would predict: the surrogate objective does not match the true reward landscape, learned value estimators fail to fit the true value function, and gradient estimates poorly correlate with the "true" gradient. The mismatch between predicted and empirical behavior we uncover highlights our poor understanding of current methods, and indicates the need to move beyond current benchmark-centric evaluation methods.},
archiveprefix = {arXiv},
eprint = {1811.02553},
file = {:http\://arxiv.org/pdf/1811.02553v4:PDF},
keywords = {cs.LG, cs.NE, cs.RO, stat.ML},
primaryclass = {cs.LG},
}

@Comment{jabref-meta: databaseType:bibtex;}
